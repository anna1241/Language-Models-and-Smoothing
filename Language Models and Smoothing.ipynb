{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440efc5f",
      "metadata": {
        "scrolled": false,
        "id": "440efc5f"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import sys\n",
        "import random\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b123698e",
      "metadata": {
        "id": "b123698e"
      },
      "outputs": [],
      "source": [
        "def readFileToCorpus(f):\n",
        "    \"\"\" Reads in the text file f which contains one sentence per line.\n",
        "    \"\"\"\n",
        "    if os.path.isfile(f):\n",
        "        file = open(f, \"r\") # open the input file in read-only mode\n",
        "        i = 0 # this is just a counter to keep track of the sentence numbers\n",
        "        corpus = [] # this will become a list of sentences\n",
        "        print(\"Reading file \", f)\n",
        "        for line in file:\n",
        "            i += 1\n",
        "            sentence = line.split() # split the line into a list of words\n",
        "            #append this lis as an element to the list of sentences\n",
        "            corpus.append(sentence)\n",
        "            if i % 1000 == 0:\n",
        "    \t#print a status message: str(i) turns int i into a string\n",
        "    \t#so we can concatenate it\n",
        "                sys.stderr.write(\"Reading sentence \" + str(i) + \"\\n\")\n",
        "        #endif\n",
        "    #endfor\n",
        "        return corpus\n",
        "    else:\n",
        "    #ideally we would throw an exception here, but this will suffice\n",
        "        print(\"Error: corpus file \", f, \" does not exist\")\n",
        "        sys.exit() # exit the script\n",
        "    #endif\n",
        "#enddef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89511101",
      "metadata": {
        "id": "89511101"
      },
      "outputs": [],
      "source": [
        "# Preprocess the corpus\n",
        "def preprocess(corpus):\n",
        "    #find all the rare words\n",
        "    freqDict = defaultdict(int)\n",
        "    for sen in corpus:\n",
        "\t    for word in sen:\n",
        "\t       freqDict[word] += 1\n",
        "\t#endfor\n",
        "    #endfor\n",
        "\n",
        "    #replace rare words with unk\n",
        "    for sen in corpus:\n",
        "        for i in range(0, len(sen)):\n",
        "            word = sen[i]\n",
        "            print(word)\n",
        "            print(freqDict[word])\n",
        "            if freqDict[word] < 2:\n",
        "\n",
        "                sen[i] = UNK\n",
        "\t    #endif\n",
        "\t#endfor\n",
        "    #endfor\n",
        "\n",
        "    #bookend the sentences with start and end tokens\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    #endfor\n",
        "\n",
        "    return corpus\n",
        "#enddef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ef0456",
      "metadata": {
        "id": "21ef0456"
      },
      "outputs": [],
      "source": [
        "def preprocessTest(vocab, corpus):\n",
        "    #replace test words that were unseen in the training with unk\n",
        "    for sen in corpus:\n",
        "        for i in range(0, len(sen)):\n",
        "            word = sen[i]\n",
        "            if word not in vocab:\n",
        "                sen[i] = UNK\n",
        "\t    #endif\n",
        "\t#endfor\n",
        "    #endfor\n",
        "\n",
        "    #bookend the sentences with start and end tokens\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    #endfor\n",
        "\n",
        "    return corpus\n",
        "#enddef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "648dc99a",
      "metadata": {
        "id": "648dc99a"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "UNK = \"UNK\"     # Unknown word token\n",
        "start = \"<s>\"   # Start-of-sentence token\n",
        "end = \"</s>\"    # End-of-sentence-token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf23c51",
      "metadata": {
        "id": "0bf23c51"
      },
      "outputs": [],
      "source": [
        "class LanguageModel:\n",
        "    def __init__(self, corpus):\n",
        "        pass\n",
        "    #enddef\n",
        "\n",
        "    def generateSentence(self):\n",
        "        pass\n",
        "    #enddef\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        pass\n",
        "    #enddef\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        pass\n",
        "    #enddef\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(0,numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "\n",
        "\t#endfor\n",
        "    #enddef\n",
        "#endclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd79747",
      "metadata": {
        "id": "dbd79747"
      },
      "outputs": [],
      "source": [
        "# Unigram language model\n",
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        super().__init__(corpus)\n",
        "        self.unigram_dist = UnigramDist(corpus)\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        while True:\n",
        "            word = self.unigram_dist.draw()\n",
        "            if word == end:\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1.0\n",
        "        for word in sen:\n",
        "            probability *= self.unigram_dist.prob(word)\n",
        "        return probability\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        total_probability = 0.0\n",
        "        total_words = 0\n",
        "        for sen in corpus:\n",
        "            for word in sen:\n",
        "                total_probability += math.log(self.unigram_dist.prob(word) + 1e-10)\n",
        "                total_words += 1\n",
        "        perplexity = - (total_probability / total_words)\n",
        "        return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df31739",
      "metadata": {
        "id": "4df31739"
      },
      "outputs": [],
      "source": [
        "#Smoothed unigram language model\n",
        "class SmoothedUnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        super().__init__(corpus)\n",
        "        self.unigram_dist = UnigramDist(corpus)\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        while True:\n",
        "            word = self.unigram_dist.draw()\n",
        "            if word == end:\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1.0\n",
        "        for word in sen:\n",
        "            probability *= self.laplaceSmoothing(word)\n",
        "        return probability\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        total_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sen in corpus:\n",
        "            for word in sen:\n",
        "                total_prob += math.log(self.laplaceSmoothing(word))\n",
        "                total_words += 1\n",
        "        perplexity = - (total_prob / total_words)\n",
        "        return perplexity\n",
        "\n",
        "    def laplaceSmoothing(self, word):\n",
        "        return (self.unigram_dist.counts[word] + 1.0) / (self.unigram_dist.total + len(self.unigram_dist.counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0332df2d",
      "metadata": {
        "id": "0332df2d"
      },
      "outputs": [],
      "source": [
        "# Unsmoothed bigram language model\n",
        "class BigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        super().__init__(corpus)\n",
        "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.unigram_dist = UnigramDist(corpus)  # Add this line to reference the unigram distribution\n",
        "        self.calculateBigramCounts(corpus)\n",
        "\n",
        "    def calculateBigramCounts(self, corpus):\n",
        "        for sen in corpus:\n",
        "            for i in range(1, len(sen)):\n",
        "                self.bigram_counts[sen[i - 1]][sen[i]] += 1\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        prev_word = start\n",
        "        while True:\n",
        "            next_word_candidates = list(self.bigram_counts[prev_word].keys())\n",
        "            next_word = random.choice(next_word_candidates)\n",
        "            if next_word == end:\n",
        "                break\n",
        "            sentence.append(next_word)\n",
        "            prev_word = next_word\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1.0\n",
        "        for i in range(1, len(sen)):\n",
        "            bigram_count = self.bigram_counts[sen[i - 1]][sen[i]]\n",
        "            if self.unigram_dist.counts[sen[i - 1]] > 0:\n",
        "                probability *= bigram_count / self.unigram_dist.counts[sen[i - 1]]\n",
        "            else:\n",
        "                probability = 0.0\n",
        "                break\n",
        "        return probability\n",
        "\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        total_log_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sen in corpus:\n",
        "            prev_word = '<s>'\n",
        "            for word in sen:\n",
        "                if word in self.bigram_counts[prev_word]:\n",
        "                    bigram_count = self.bigram_counts[prev_word][word]\n",
        "                    denominator = self.unigram_dist.counts[prev_word]\n",
        "                    if denominator > 0:\n",
        "                        total_log_prob += -1.0 * math.log(bigram_count / denominator, 2)\n",
        "                        total_words += 1\n",
        "                prev_word = word\n",
        "        perplexity = 2 ** (total_log_prob / total_words) if total_words > 0 else float('inf')\n",
        "        return perplexity\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd5b61d",
      "metadata": {
        "id": "edd5b61d"
      },
      "outputs": [],
      "source": [
        "class SmoothedBigramModelKN(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        super().__init__(corpus)\n",
        "        self.unigram_dist = UnigramDist(corpus)\n",
        "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.calculateBigramCounts(corpus)\n",
        "\n",
        "    def calculateBigramCounts(self, corpus):\n",
        "        for sen in corpus:\n",
        "            for i in range(1, len(sen)):\n",
        "                self.bigram_counts[sen[i - 1]][sen[i]] += 1\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = [start]\n",
        "        prev_word = start\n",
        "        while True:\n",
        "            next_word_candidates = list(self.bigram_counts[prev_word].keys())\n",
        "            probabilities = [self.linearProbability(prev_word, next_word, 0.5, 0.5) for next_word in next_word_candidates]\n",
        "            next_word = random.choices(next_word_candidates, weights=probabilities)[0]\n",
        "            if next_word == end:\n",
        "                break\n",
        "            sentence.append(next_word)\n",
        "            prev_word = next_word\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        probability = 1.0\n",
        "        for i in range(1, len(sen)):\n",
        "            probability *= self.linearProbability(sen[i - 1], sen[i], 0.5, 0.5)\n",
        "        return probability\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        total_prob = 0.0\n",
        "        total_words = 0\n",
        "        for sen in corpus:\n",
        "            for i in range(1, len(sen)):\n",
        "                probability = self.linearProbability(sen[i - 1], sen[i], 0.5, 0.5)\n",
        "                if probability > 0:\n",
        "                    total_prob += math.log(probability)\n",
        "                    total_words += 1\n",
        "        perplexity = 2 ** (-total_prob / total_words) if total_words > 0 else float('inf')\n",
        "        return perplexity\n",
        "\n",
        "\n",
        "    def linearProbability(self, prev_word, current_word, lambda1, lambda2):\n",
        "        unigram_prob = self.unigram_dist.prob(current_word)\n",
        "        if prev_word in self.unigram_dist.counts and self.unigram_dist.counts[prev_word] > 0:\n",
        "            if prev_word in self.bigram_counts and current_word in self.bigram_counts[prev_word]:\n",
        "                bigram_count = self.bigram_counts[prev_word][current_word]\n",
        "                bigram_prob = bigram_count / self.unigram_dist.counts[prev_word]\n",
        "            else:\n",
        "                bigram_prob = 0.0\n",
        "        else:\n",
        "            bigram_prob = 0.0\n",
        "        return lambda1 * unigram_prob + lambda2 * bigram_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2663a5e4",
      "metadata": {
        "id": "2663a5e4"
      },
      "outputs": [],
      "source": [
        "# Read the corpus from a file and preprocess it\n",
        "def readFileToCorpus(filename):\n",
        "    corpus = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            sentence = line.split()\n",
        "            corpus.append(sentence)\n",
        "    return corpus\n",
        "\n",
        "def preprocess(corpus):\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, \"<s>\")\n",
        "        sen.append(\"</s>\")\n",
        "    return corpus\n",
        "\n",
        "def preprocessTest(vocab, corpus):\n",
        "    for sen in corpus:\n",
        "        for i in range(len(sen)):\n",
        "            word = sen[i]\n",
        "            if word not in vocab:\n",
        "                sen[i] = \"<UNK>\"\n",
        "    return corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7999db10",
      "metadata": {
        "id": "7999db10"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "start = \"<s>\"\n",
        "end = \"</s>\"\n",
        "UNK = \"<UNK>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afce6de6",
      "metadata": {
        "id": "afce6de6",
        "outputId": "58252183-d943-40fe-8eb8-936658be8781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram Perplexity Negative Test Corpus: 7.332797738558924\n",
            "Unigram Perplexity Positive Test Corpus: 7.32406751604652\n",
            "Smoothed Unigram Perplexity Negative Test Corpus: 7.045155383676817\n",
            "Smoothed Unigram Perplexity Positive Test Corpus: 7.030083806521279\n",
            "Bigram Perplexity Negative Test Corpus: 67.03464496532278\n",
            "Bigram Perplexity Positive Test Corpus: 64.01204730838485\n",
            "Smoothed Bigram Perplexity Negative Test Corpus: 63.30171400967248\n",
            "Smoothed Bigram Perplexity Positive Test Corpus: 59.80622758765212\n"
          ]
        }
      ],
      "source": [
        "#-------------------------------------------\n",
        "# The main routine\n",
        "#-------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    #read your corpora\n",
        "    trainCorpus = readFileToCorpus('train.txt')\n",
        "    trainCorpus = preprocess(trainCorpus)\n",
        "\n",
        "    posTestCorpus = readFileToCorpus('pos_test.txt')\n",
        "    negTestCorpus = readFileToCorpus('neg_test.txt')\n",
        "\n",
        "    vocab = set(word for sent in trainCorpus for word in sent)\n",
        "\n",
        "    posTestCorpus = preprocessTest(vocab, posTestCorpus)\n",
        "    negTestCorpus = preprocessTest(vocab, negTestCorpus)\n",
        "\n",
        "    unigramModel = UnigramModel(trainCorpus)\n",
        "    smoothedUnigramModel = SmoothedUnigramModel(trainCorpus)\n",
        "    bigramModel = BigramModel(trainCorpus)\n",
        "    smoothedBigramModel = SmoothedBigramModelKN(trainCorpus)\n",
        "\n",
        "    unigramModel.generateSentencesToFile(20, 'unigram_output.txt')\n",
        "    smoothedUnigramModel.generateSentencesToFile(20, 'smooth_unigram_output.txt')\n",
        "    bigramModel.generateSentencesToFile(20, 'bigram_output.txt')\n",
        "    smoothedBigramModel.generateSentencesToFile(20, 'smooth_bigram_kn_output.txt')\n",
        "\n",
        "    unigram_perplexity_negTest = unigramModel.getCorpusPerplexity(negTestCorpus)\n",
        "    unigram_perplexity_posTest = unigramModel.getCorpusPerplexity(posTestCorpus)\n",
        "    smoothedUnigram_perplexity_negTest = smoothedUnigramModel.getCorpusPerplexity(negTestCorpus)\n",
        "    smoothedUnigram_perplexity_posTest = smoothedUnigramModel.getCorpusPerplexity(posTestCorpus)\n",
        "\n",
        "    bigram_perplexity_negTest = bigramModel.getCorpusPerplexity(negTestCorpus)\n",
        "    bigram_perplexity_posTest = bigramModel.getCorpusPerplexity(posTestCorpus)\n",
        "    smoothedBigram_perplexity_negTest = smoothedBigramModel.getCorpusPerplexity(negTestCorpus)\n",
        "    smoothedBigram_perplexity_posTest = smoothedBigramModel.getCorpusPerplexity(posTestCorpus)\n",
        "\n",
        "    print('Unigram Perplexity Negative Test Corpus:', unigram_perplexity_negTest)\n",
        "    print('Unigram Perplexity Positive Test Corpus:', unigram_perplexity_posTest)\n",
        "    print('Smoothed Unigram Perplexity Negative Test Corpus:', smoothedUnigram_perplexity_negTest)\n",
        "    print('Smoothed Unigram Perplexity Positive Test Corpus:', smoothedUnigram_perplexity_posTest)\n",
        "\n",
        "    print('Bigram Perplexity Negative Test Corpus:', bigram_perplexity_negTest)\n",
        "    print('Bigram Perplexity Positive Test Corpus:', bigram_perplexity_posTest)\n",
        "    print('Smoothed Bigram Perplexity Negative Test Corpus:', smoothedBigram_perplexity_negTest)\n",
        "    print('Smoothed Bigram Perplexity Positive Test Corpus:', smoothedBigram_perplexity_posTest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016757c4",
      "metadata": {
        "id": "016757c4"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "#In the Unigram model, how long a sentence is depends on how often individual words appear in a group of sentences\n",
        "#we use to teach the computer. But in the Bigram model, the computer decides what word comes next in a sentence based on\n",
        "#the chance of it coming after the previous word. This can make the sentences more organized compared to the Unigram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed3b6f3",
      "metadata": {
        "id": "6ed3b6f3"
      },
      "outputs": [],
      "source": [
        "#Question 2\n",
        "#Yes, the computer models give very different chances to different groups of sentences. This happens because each model\n",
        "#looks at different parts of how language works. The Unigram model looks at each word on its own, which might make\n",
        "#sentences that are not very realistic. But the Bigram model looks at how words are connected, making sentences that\n",
        "#seem more real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3fa7707",
      "metadata": {
        "id": "a3fa7707"
      },
      "outputs": [],
      "source": [
        "#Question 3\n",
        "bigramModel.generateSentencesToFile(5, 'bigram_output2.txt')\n",
        "smoothedBigramModel.generateSentencesToFile(5, 'smoothBigramkn_output2.txt')\n",
        "#In my view, the Smoothed Bigram model creates better sentences. It uses Linear Interpolation smoothing to\n",
        "#deal with word pairs that haven't been seen before, making the sentences more realistic compared to the basic bigram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd53ddf",
      "metadata": {
        "id": "cfd53ddf"
      },
      "outputs": [],
      "source": [
        "#Question 4\n",
        "#Unigram Perplexity Negative Test Corpus: 7.332797738558924\n",
        "#Unigram Perplexity Positive Test Corpus: 7.32406751604652\n",
        "#Smoothed Unigram Perplexity Negative Test Corpus: 7.045155383676817\n",
        "#Smoothed Unigram Perplexity Positive Test Corpus: 7.030083806521279\n",
        "#Bigram Perplexity Negative Test Corpus: 67.03464496532278\n",
        "#Bigram Perplexity Positive Test Corpus: 64.01204730838485\n",
        "#Smoothed Bigram Perplexity Negative Test Corpus: 63.30171400967248\n",
        "#Smoothed Bigram Perplexity Positive Test Corpus: 59.80622758765212\n",
        "\n",
        "# The Bigram Model has a higher perplexity, indicating that the language model finds it more\n",
        "#challenging to predict word sequences in this corpus"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}